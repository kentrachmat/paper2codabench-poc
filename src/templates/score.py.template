#!/usr/bin/env python3
"""
Scoring Program for {{task_name}}
Auto-generated from Croissant Task

Primary Metric: {{primary_metric}}
Task Type: {{task_type}}
"""
import os
import sys
import json
import pandas as pd
from pathlib import Path

# Import metrics module
from metrics import compute_metrics


def main():
    """Main scoring function"""
    # Standard Codabench directory structure
    input_dir = Path('/app/input')
    output_dir = Path('/app/output')
    reference_dir = Path('/app/reference_data')
    scores_dir = Path('/app/scores')

    # For local testing, use different paths
    if not input_dir.exists():
        input_dir = Path('output')
        reference_dir = Path('reference_data')
        scores_dir = Path('scores')

    scores_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("Scoring Program: {{task_name}}")
    print("=" * 60)

    # Load predictions
    predictions_file = input_dir / "predictions.csv"
    if not predictions_file.exists():
        print(f"ERROR: Predictions file not found: {predictions_file}")
        sys.exit(1)

    try:
        predictions_df = pd.read_csv(predictions_file)
        print(f"  Loaded {len(predictions_df)} predictions")
    except Exception as e:
        print(f"ERROR loading predictions: {e}")
        sys.exit(1)

    # Load reference data
    reference_file = reference_dir / "reference.csv"
    if not reference_file.exists():
        print(f"ERROR: Reference file not found: {reference_file}")
        sys.exit(1)

    try:
        reference_df = pd.read_csv(reference_file)
        print(f"  Loaded {len(reference_df)} reference labels")
    except Exception as e:
        print(f"ERROR loading reference: {e}")
        sys.exit(1)

    # Merge on ID
    merged = reference_df.merge(
        predictions_df,
        on='id',
        how='inner',
        suffixes=('_true', '_pred')
    )

    if len(merged) == 0:
        print("ERROR: No matching IDs between predictions and reference")
        sys.exit(1)

    if len(merged) < len(reference_df):
        missing = len(reference_df) - len(merged)
        print(f"WARNING: {missing} reference IDs not found in predictions")

    print(f"  Matched {len(merged)} predictions with reference")

    # Extract true and predicted values
    y_true = merged['{{target_column}}_true'].values
    y_pred = merged['{{target_column}}_pred'].values

    # Compute metrics
    print("\nComputing metrics...")
    try:
        scores = compute_metrics(y_true, y_pred, task_type='{{task_type}}')
        print("  Metrics computed successfully")
    except Exception as e:
        print(f"ERROR computing metrics: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # Display results
    print("\n" + "=" * 60)
    print("RESULTS:")
    print("=" * 60)
    for metric_name, metric_value in scores.items():
        print(f"  {metric_name}: {metric_value:.6f}")
    print("=" * 60)

    # Save scores
    scores_file = scores_dir / "scores.json"
    with open(scores_file, 'w') as f:
        json.dump(scores, f, indent=2)

    print(f"\n  Scores saved to {scores_file}")
    print("\nScoring completed successfully")


if __name__ == "__main__":
    main()
